{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "# source: https://www.kaggle.com/allunia/patterns-of-colorectal-cancer-wally\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Import classifiers used:\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Data Transformation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer\n",
    "\n",
    "\n",
    "# Import ovo and ovr related stuff\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Pre-processing Data:\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, cross_val_predict, GridSearchCV, KFold\n",
    "\n",
    "# Import Metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_curve, classification_report, multilabel_confusion_matrix, confusion_matrix, plot_confusion_matrix, plot_roc_curve, precision_score, recall_score\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import kerastuner as kt\n",
    "from tensorflow import keras\n",
    "\n",
    "#Base Libraries\n",
    "import random\n",
    "\n",
    "# Custom Transformer for Y\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU Detected. TF good to go\n"
     ]
    }
   ],
   "source": [
    "# Ensure that GPU is detected\n",
    "assert(tf.config.experimental.list_physical_devices('GPU') is not None), 'GPU not detected'\n",
    "print('GPU Detected. TF good to go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data from CSV File & Do Preprocessing required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../01_Data_Files/hmnist_64_64_L.csv', engine='c') # read 64x64 gray scale images as csv\n",
    "dict_class_names = {1: 'Tumor', 2:'Stroma', 3:'Complex', 4:'Lymphoma', 5:'Debris', 6:'Mucosa', 7:'Adipose', 8:'Empty'}\n",
    "\n",
    "# Adds a column with name of the label based on the integer value\n",
    "df['label_name'] = df['label'].map(dict_class_names)\n",
    "\n",
    "# \n",
    "class_names = ['Tumor', 'Stroma', 'Complex', 'Lymphoma', 'Debris', 'Mucosa', 'Adipose', 'Empty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['label', 'label_name'], axis=1).values\n",
    "y = df.loc[:, 'label_name'].values.reshape(-1,1)\n",
    "\n",
    "assert len(X) == len(y), 'X & y length mismatch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MinMax Scaler to scale the values of X from 0-255 to between 0 and 1. \n",
    "\n",
    "The min max scaler uses the following formula to calculate the scaled values:\n",
    "\n",
    "$$ x^{\\prime} = \\frac{x-min(x)}{max(x)-min(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Values of X via a MinMaxScaler\n",
    "# We know that our input can vary between 0 and 255\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Create a fake array with values between 0 and 255 to use for fitting min max scaler\n",
    "scaling_array = np.append(\n",
    "    [np.zeros_like(X[0], dtype=np.int16)], [np.full_like(X[0], 255, dtype=np.int16)], axis=0\n",
    ")\n",
    "min_max_scaler.fit(scaling_array)\n",
    "X_scaled = min_max_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode Y values for Tensorflow Usage & Label Encode for Sklearn Usage\n",
    "\n",
    "To learn more about one hot encoding:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=one%20hot%20encode#sklearn.preprocessing.OneHotEncoder\n",
    "\n",
    "To learn more about label encoding:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html?highlight=encoder#sklearn.preprocessing.LabelEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "One Hot Encoded Y Shape:  (5000, 8)\nLabel Encoded Y Shape: (5000,), Max Value: 7, Min Value: 0\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encode y_values:\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = one_hot_encoder.fit_transform(y)\n",
    "print('One Hot Encoded Y Shape: ', y_one_hot.shape)\n",
    "\n",
    "# Label Encoding to conver String labels to integer labels:\n",
    "label_encoder = LabelEncoder()\n",
    "y_int_encode = label_encoder.fit_transform(y)\n",
    "print(f'Label Encoded Y Shape: {y_int_encode.shape}, Max Value: {y_int_encode.max()}, Min Value: {y_int_encode.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make sure that that distribution of our data hasn't changed significantly due to scaling. \n",
    "# Mostly a sanity check\n",
    "# fig, axs = plt.subplots(4, 4, figsize=(22,27))\n",
    "# for i in range(4):\n",
    "\n",
    "#     # Show Original Images\n",
    "#     original_image = np.reshape(X_scaled[i], (64,64))\n",
    "#     axs[0,i].imshow(original_image, cmap='gray')\n",
    "#     label_image = y[i]\n",
    "#     axs[0,i].set_title(f'Original Image: {label_image}')\n",
    "#     axs[0,i].grid(False)\n",
    "\n",
    "#     # Show histogram of values in image:\n",
    "#     sns.distplot(original_image, ax=axs[1,i])\n",
    "#     axs[1,i].set_title('Original Image Distribution')\n",
    "#     axs[1,i].set_ylabel('')\n",
    "\n",
    "#     # Show Scaled Images:\n",
    "#     scaled_image = np.reshape(X_scaled[i], (64,64))\n",
    "#     axs[2,i].imshow(scaled_image, cmap='gray')\n",
    "#     label_image = y[i]\n",
    "#     axs[2,i].set_title(f'MinMax Scaled Image: {label_image}')\n",
    "#     axs[2,i].grid(False)\n",
    "\n",
    "#     # Show histogram of values in image:\n",
    "#     sns.distplot(scaled_image, ax=axs[3,i])\n",
    "#     axs[3,i].set_title('MinMax Image Distribution')\n",
    "#     axs[3,i].set_ylabel('')\n",
    "\n",
    "# plt.savefig('Image Distributions.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Nested CV\n",
    "\n",
    "Nested CV allows us to do both hyper parameter tuning and classifier performance comparision without producing an overly-optimistic score as in the case of non-nested CV. This happens when information leaks into the model since we use the same data to not only do hyper paramter tuning but also evaluation. \n",
    "\n",
    "We will use evolutionary algorithms to find the best parameters instead of relying on grid search. This is done in the interest of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters for each model:\n",
    "\n",
    "Create a parameter grid for evolutionary search: \n",
    "The structure of the grid wil be: {clf_name:{param_name:value, param_name:value}, clf_name:{param_name:value...}}\n",
    "\n",
    "Nested dictionaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network:\n",
    "# Architecture: Input - Conv2D, Conv 2D, Conv2D, Flatten, Dense, Dense, Dense - Output\n",
    "# Source: https://www.kaggle.com/efeergun96/colorectal-histology-prediction-by-cnn\n",
    "def create_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Reshape(\n",
    "        target_shape= (64,64,1), \n",
    "        input_shape = (4096,),\n",
    "    ))\n",
    "\n",
    "    model.add(Conv2D(\n",
    "        filters = 128, \n",
    "        kernel_size = (5,5), \n",
    "        padding = 'same',\n",
    "        activation = 'relu' \n",
    "    ))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3,3),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3,3),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate= hp_learning_rate) , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['k_neighbors_clf'] = {'n_neighbors':np.linspace(3, 20, num=3,dtype=np.int),\n",
    "                                'weights':['distance'],\n",
    "                                'leaf_size': np.linspace(10, 30, num=3, dtype=np.int),\n",
    "                                'p':[1]}\n",
    "\n",
    "param_grid['gaussianb_clf'] = {'var_smoothing':[1e-9]}\n",
    "param_grid['svm_clf_ovr']  = {  'C'     : np.logspace(-5, 3, num=5, base=10),\n",
    "                            'gamma' : np.logspace(-9, 3, num=5, base=10), \n",
    "                            'kernel': ['rbf'],\n",
    "                            'random_state':[42],\n",
    "                            'decision_function_shape':['ovr','ovo']}\n",
    "\n",
    "param_grid['svm_clf_ovo']  = {  'C'     : np.logspace(-5, 5, num=3, base=10),\n",
    "                            'gamma' : np.logspace(-9, 9, num=3, base=10), \n",
    "                            'kernel': ['rbf'],\n",
    "                            'random_state':[42],\n",
    "                            'decision_function_shape':['ovo']}\n",
    "\n",
    "param_grid['hist_grad_boost_clf'] = {'loss':['categorical_crossentropy'],\n",
    "                                    'learning_rate':[0.01,0.1,0.5],\n",
    "                                    'l2_regularization':[0, 1, 10],\n",
    "                                    'random_state':[42]}\n",
    "\n",
    "param_grid['dummy_clf'] = {'strategy': ['stratified'],\n",
    "                            'random_state':[42]}\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neighbors_clf = KNeighborsClassifier()\n",
    "gaussianb_clf = GaussianNB()\n",
    "\n",
    "svm_clf_ovo = svm.SVC()\n",
    "svm_clf_ovr = svm.SVC()\n",
    "\n",
    "\n",
    "hist_grad_boost_clf = HistGradientBoostingClassifier()\n",
    "\n",
    "# Used for a baseline. \n",
    "dummy_clf = DummyClassifier()\n",
    "\n",
    "tuner = kt.Hyperband(create_model, \n",
    "                                    objective= 'val_accuracy',\n",
    "                                    max_epochs=20,\n",
    "                                    factor=3,\n",
    "                                    directory = 'keras-model-logs',\n",
    "                                    project_name = 'ece740-project1'\n",
    "                                    )\n",
    "\n",
    "clfs = {\n",
    "    'k_neighbors_clf' : k_neighbors_clf,\n",
    "    'gaussianb_clf' : gaussianb_clf,\n",
    "    'svm_clf_ovr' : svm_clf_ovr,\n",
    "    'svm_clf_ovo' : svm_clf_ovo,\n",
    "    'hist_grad_boost_clf' : hist_grad_boost_clf,\n",
    "    'dummy_clf': dummy_clf,\n",
    "    'tuner':tuner\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'acc':'accuracy',\n",
    "    'prec_macro':'precision_macro',\n",
    "    'rec_macro':'recall_macro',\n",
    "    'f1_macro':'f1_macro',\n",
    "}\n",
    "\n",
    "def custom_scorer(y_true, y_pred, score_dict):\n",
    "    if not score_dict:\n",
    "        score_dict['test_acc'] = []\n",
    "        score_dict['test_prec_macro'] = []\n",
    "        score_dict['test_rec_macro'] = []\n",
    "        score_dict['test_f1_macro'] = []\n",
    "    \n",
    "    score_dict['test_acc'].append(accuracy_score(y_true, y_pred))\n",
    "    score_dict['test_prec_macro'].append(precision_score(y_true, y_pred, average='macro'))\n",
    "    score_dict['test_rec_macro'].append(recall_score(y_true, y_pred, average='macro'))\n",
    "    score_dict['test_f1_macro'].append(f1_score(y_true, y_pred, average='macro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trial 3 Complete [00h 00m 03s]\n",
      "val_accuracy: 0.3987206816673279\n",
      "\n",
      "Best val_accuracy So Far: 0.6130064129829407\n",
      "Total elapsed time: 00h 00m 10s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "Epoch 1/20\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9275 - accuracy: 0.6039 - val_loss: 2.8926 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.7906 - accuracy: 0.6673 - val_loss: 2.4432 - val_accuracy: 0.0347\n",
      "Epoch 3/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.7444 - accuracy: 0.6815 - val_loss: 2.6628 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.7032 - accuracy: 0.7090 - val_loss: 2.0332 - val_accuracy: 0.0480\n",
      "Epoch 5/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.6549 - accuracy: 0.7250 - val_loss: 1.8046 - val_accuracy: 0.1387\n",
      "Epoch 6/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.6271 - accuracy: 0.7496 - val_loss: 1.8907 - val_accuracy: 0.1520\n",
      "Epoch 7/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.6934 - accuracy: 0.7031 - val_loss: 2.2608 - val_accuracy: 0.1200\n",
      "Epoch 8/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5576 - accuracy: 0.7816 - val_loss: 1.9801 - val_accuracy: 0.2080\n",
      "Epoch 9/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5520 - accuracy: 0.7834 - val_loss: 2.3428 - val_accuracy: 0.1493\n",
      "Epoch 10/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5216 - accuracy: 0.7831 - val_loss: 1.9319 - val_accuracy: 0.2587\n",
      "Epoch 11/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.5259 - accuracy: 0.7970 - val_loss: 2.9071 - val_accuracy: 0.0320\n",
      "Epoch 12/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4083 - accuracy: 0.8468 - val_loss: 1.6984 - val_accuracy: 0.3653\n",
      "Epoch 13/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4063 - accuracy: 0.8510 - val_loss: 2.7326 - val_accuracy: 0.1147\n",
      "Epoch 14/20\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.4140 - accuracy: 0.8483 - val_loss: 1.9913 - val_accuracy: 0.2213\n",
      "Epoch 15/20\n",
      " 78/106 [=====================>........] - ETA: 0s - loss: 0.3997 - accuracy: 0.8542"
     ]
    }
   ],
   "source": [
    "# Dictionary will contain scores for each of the \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "nested_scores = {}\n",
    "non_nested_scores = {}\n",
    "# Using F1-macro since we have a balanced data set. \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "\n",
    "for key, value in clfs.items():\n",
    "    print(f'Fitting and testing {key} classifier')\n",
    "    outer_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=RANDOM_SEED)\n",
    "    inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    # Formats the label (y) as either integer encoding or one-hot encoding for non-cnn clfs \n",
    "    # and cnn-clfs accordingly. \n",
    "    if key == 'tuner':\n",
    "        y_current = y_one_hot\n",
    "        n_jobs_param = 1\n",
    "\n",
    "        nested_scores[key] = {}\n",
    "\n",
    "        for train_idx, test_idx in tqdm_notebook(outer_cv.split(X_scaled, y_int_encode)):\n",
    "            # create fake training target with integer encode to be used for inner split. SKFold does not like one-hot-encoded labels. \n",
    "            fake_train_target_ = y_int_encode[train_idx]\n",
    "            \n",
    "            train_data, test_data = X_scaled[train_idx], X_scaled[test_idx]\n",
    "            train_target, test_target = y_current[train_idx], y_current[test_idx]\n",
    "\n",
    "            # Do parameter tuning with validation/inner cv. \n",
    "\n",
    "            for i, (train_idx_inner, val_idx) in tqdm_notebook(enumerate(inner_cv.split(train_data, fake_train_target_))):\n",
    "                train_data_inner, validate_data = train_data[train_idx_inner], train_data[val_idx]\n",
    "                train_target_inner, validate_target = train_target[train_idx_inner], train_target[val_idx]\n",
    "\n",
    "                value.search(train_data_inner, train_target_inner, epochs=20, validation_data = (validate_data, validate_target))\n",
    "\n",
    "\n",
    "            best_hps = value.get_best_hyperparameters(num_trials = 1)[0]\n",
    "            best_model = value.get_best_models(num_models=1)[0]\n",
    "\n",
    "            history = best_model.fit(train_data, train_target, epochs=20, verbose=1, validation_split=0.1)\n",
    "\n",
    "            predictions = best_model.predict(test_data)\n",
    "\n",
    "            predictions = one_hot_encoder.inverse_transform(predictions)\n",
    "            test_target_oh = one_hot_encoder.inverse_transform(test_target)\n",
    "\n",
    "            # nested_scores[key].append(f1_score(test_target_oh, predictions, average='macro'))\n",
    "            # Do testing & append scores\n",
    "            custom_scorer(test_target_oh, predictions, nested_scores[key])\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        y_current = y_int_encode\n",
    "        n_jobs_param = -1\n",
    "\n",
    "        clf = GridSearchCV(estimator=value, \n",
    "                        param_grid=param_grid[key], \n",
    "                        scoring='f1_macro',\n",
    "                        cv=inner_cv, \n",
    "                        n_jobs=n_jobs_param,\n",
    "                        verbose=1,\n",
    "                        refit=True)\n",
    "\n",
    "        clf.fit(X_scaled, y_current)\n",
    "\n",
    "        print(f'Parameter Optimization Complete for {key}, best params: \\n{clf.best_params_}')\n",
    "        nested_scores[key] = cross_validate(\n",
    "            clf,\n",
    "            X=X_scaled,\n",
    "            y=y_current,\n",
    "            scoring=scoring,\n",
    "            cv=outer_cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('results.json', 'w') as write_file:\n",
    "    json.dump(nested_scores, write_file, indent=4)\n",
    "\n",
    "df_scores = pd.DataFrame(nested_scores)\n",
    "df_scores.to_csv('nest_scores.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ece-740-sm': conda)",
   "metadata": {
    "interpreter": {
     "hash": "840e7341ca500d356bb47c00147fed75f20a8b9b4885feed88ba285e965d9439"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}